# -*- coding: utf-8 -*-
"""05_Claudia50_Tests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/otausendschoen/Image_Classification_Diffusion-Augmentation/blob/main/05_Claudia50_Tests.ipynb

<a href="https://colab.research.google.com/github/otausendschoen/Image_Classification_Diffusion-Augmentation/blob/main/03_10%25Synthetic_Classification.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""


import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, ConcatDataset, Subset
from torchvision import transforms, models
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score
import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from sklearn.model_selection import train_test_split
import random
from collections import Counter, defaultdict
from torchvision.datasets import ImageFolder
import pandas as pd
#import seaborn as sns
import zipfile
import torch
print(torch.cuda.is_available())
print(torch.version.cuda)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


print(f"current cwd: {os.getcwd()}")
import os
######
zip_path = "./data/eurosat-dataset.zip"
target_dir = "./data/"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(target_dir)
    print(f" Extracted '{zip_path}' into '{target_dir}'")


#####
print("Loading dataset...")
#dataset = load_dataset("jonathan-roberts1/EuroSAT") #ALTERNATIVE
dataset = load_dataset(
    "imagefolder",
    data_dir="./data/eurosat-dataset/6/EuroSAT",
)

print(dataset)


##################################################################################################################################################################
# ADJUST THIS TO THE PROPORTION OF THE DATASET YOU WANT TO USE
# FOR EXAMPLE shuffle and select a subset of the training data (10% for quick testing)
dataset["train"] = dataset["train"].shuffle(seed=42).select(
    range(len(dataset["train"]) // 10)
)


# Create train-validation split
train_val_dataset = dataset["train"]
train_indices, val_indices = train_test_split(
    range(len(train_val_dataset)),
    test_size=0.2,
    stratify=[item['label'] for item in train_val_dataset],
    random_state=8
)

class ResNet18Classifier(nn.Module):
    def __init__(self, num_classes=10, pretrained=True):
        super(ResNet18Classifier, self).__init__()

        try:
            self.model = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)
        except:
            self.model = models.resnet18(pretrained=pretrained)

        # Replacing the final fully connected layer
        in_features = self.model.fc.in_features
        self.model.fc = nn.Linear(in_features, num_classes)

    def forward(self, x):
        return self.model(x)

"""In our implementation, we created a custom EuroSATDataset class that inherits from PyTorch's Dataset, providing an efficient interface between our raw satellite imagery and the training pipeline. This class handles the application of image transformations and maintains the train-validation split through indexing, allowing flexible access to different subsets of the data. We then instantiated separate training and validation datasets with their respective transformations, and wrapped them in DataLoaders with a batch size of 32. The training loader incorporates shuffling to improve model generalization, while the validation loader maintains deterministic ordering. This approach enables efficient batched processing and parallelized data loading, which is essential for training deep convolutional networks like Wide ResNet50 on large image datasets."""

# Create PyTorch datasets
class EuroSATDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, indices=None, transform=None):
        self.dataset = dataset
        self.indices = indices if indices is not None else range(len(dataset))
        self.transform = transform

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        sample = self.dataset[self.indices[idx]]
        image = sample['image']
        label = sample['label']

        if self.transform:
            image = self.transform(image)

        return image, label


# Training function
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    for images, labels in tqdm(dataloader, desc="Training"):
        images, labels = images.to(device), labels.to(device)

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item() * images.size(0)
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_acc = accuracy_score(all_labels, all_preds)
    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')

    return epoch_loss, epoch_acc, epoch_f1

# Validation function
def validate(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Validation"):
            images, labels = images.to(device), labels.to(device)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Statistics
            running_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    epoch_loss = running_loss / len(dataloader.dataset)
    epoch_acc = accuracy_score(all_labels, all_preds)
    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')

    return epoch_loss, epoch_acc, epoch_f1

"""Our implementation establishes a comprehensive training framework with integrated early stopping to prevent overfitting on the EuroSAT dataset. The training loop executes up to 30 epochs, with patience set to 5 epochs for early termination if validation accuracy fails to improve. Within each epoch, we conduct a full training pass using our train_one_epoch function, followed by validation with the validate function, while systematically applying learning rate adjustments via the cosine annealing scheduler. Performance metrics including loss, accuracy, and F1-score are tracked for both training and validation sets and stored in a history dictionary for subsequent analysis. We save the model weights only when validation accuracy improves, maintaining the best-performing configuration throughout training."""

# Training loop function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,
                num_epochs=30, patience=7, save_path=None):
    best_val_acc = 0
    counter = 0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    # Store best class-wise metrics
    best_class_metrics = None
    best_confusion_matrix = None

    # Define class names
    class_names = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway',
                   'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")

        # Train
        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)

        # Validate - modify this function to return additional metrics
        val_loss, val_acc, val_f1, val_class_report, val_conf_matrix = validate_with_metrics(model, val_loader, criterion, device)

        # Update learning rate
        scheduler.step()

        # Print statistics
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")

        # Save history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # Save best model and metrics
        if val_acc > best_val_acc:
            best_val_acc = val_acc

            # Create a DataFrame from the classification report
            # Extract class-wise metrics from val_class_report
            class_metrics = []
            for i, class_name in enumerate(class_names):
                if str(i) in val_class_report:  # Check if class exists in report
                    metrics = val_class_report[str(i)]
                    class_metrics.append({
                        'Class': class_name,
                        'Precision': metrics['precision'],
                        'Recall': metrics['recall'],
                        'F1-Score': metrics['f1-score'],
                        'Support': metrics['support']
                    })

            best_class_metrics = pd.DataFrame(class_metrics)
            best_confusion_matrix = val_conf_matrix

            if save_path:
                torch.save(model.state_dict(), save_path)

                # Also save metrics
                metrics_path = save_path.replace('.pth', '_metrics.pkl')
                with open(metrics_path, 'wb') as f:
                    pickle.dump({
                        'class_metrics': best_class_metrics,
                        'confusion_matrix': best_confusion_matrix
                    }, f)

            counter = 0
        else:
            counter += 1

        # Early stopping
        if counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

    print(f"Best validation accuracy: {best_val_acc:.4f}")

    return history, best_val_acc, best_class_metrics, best_confusion_matrix

def validate_with_metrics(model, val_loader, criterion, device):
    """
    Validate the model and collect detailed class-wise metrics

    Returns:
        val_loss: Average validation loss
        val_acc: Validation accuracy
        val_f1: Validation macro F1 score
        class_report: Detailed classification report with per-class metrics
        conf_matrix: Confusion matrix
    """
    model.eval()
    running_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            _, preds = torch.max(outputs, 1)

            running_loss += loss.item() * inputs.size(0)

            # Collect predictions and labels for metrics calculation
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)

    # Calculate average loss
    val_loss = running_loss / len(val_loader.dataset)

    # Calculate accuracy
    val_acc = np.mean(all_preds == all_labels)

    # Calculate macro F1 score
    val_f1 = f1_score(all_labels, all_preds, average='macro')

    # Generate detailed classification report
    class_report = classification_report(all_labels, all_preds, output_dict=True)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(all_labels, all_preds)

    return val_loss, val_acc, val_f1, class_report, conf_matrix

def visualize_class_metrics(class_metrics, confusion_matrix, experiment_name, class_names):
    """
    Visualize class-wise metrics

    Args:
        class_metrics: DataFrame with class-wise metrics
        confusion_matrix: Confusion matrix
        experiment_name: Name of the experiment
        class_names: List of class names
    """

    # Print metrics for classes with lowest performance
    print(f"\nClass-wise metrics for {experiment_name}:")
    print(class_metrics.sort_values('F1-Score').head(3))


def create_synthetic_dataset(train_dataset, synthetic_full, synth_prop):

    # Get all original labels
    orig_labels = [label for _, label in train_dataset]
    orig_counts = Counter(orig_labels)

    # Build a map class_id -> list of synthetic indices
    class_to_syn_idxs = defaultdict(list)
    for idx, (_, lbl) in enumerate(synthetic_full):
        class_to_syn_idxs[lbl].append(idx)

    # For each class, sample synth_prop of the original count from synthetic pool
    syn_sample_idxs = []
    for cls, cnt in orig_counts.items():
        num_to_add = int(synth_prop * cnt)
        available = class_to_syn_idxs[cls]
        if len(available) < num_to_add:
            num_to_add = len(available)
        syn_sample_idxs += random.sample(available, num_to_add)

    # Create a Subset of synthetic data
    synthetic_subset = Subset(synthetic_full, syn_sample_idxs)

    # Combine original train + synthetic subset
    combined_train = ConcatDataset([train_dataset, synthetic_subset])

    print(f" → Original train samples: {len(train_dataset)}")
    print(f" → Synthetic added:       {len(synthetic_subset)}")
    print(f" → Total combined:        {len(combined_train)}")

    return combined_train


def compare_class_performance(all_class_metrics, all_confusion_matrices, class_names):
    """
    Compare class-wise performance across different experiments

    Args:
        all_class_metrics: Dictionary mapping experiment names to class metrics DataFrames
        all_confusion_matrices: Dictionary mapping experiment names to confusion matrices
        class_names: List of class names
    """
    # Create DataFrame for F1 score comparison
    f1_comparison = pd.DataFrame(index=class_names)

    for exp_name, metrics in all_class_metrics.items():
        # Extract F1 scores for each class
        f1_scores = metrics.set_index('Class')['F1-Score']
        f1_comparison[exp_name] = f1_scores

    # Calculate improvement over baseline for each class
    if 'No Augmentation' in all_class_metrics:
        baseline_metrics = all_class_metrics['No Augmentation']
        baseline_f1 = baseline_metrics.set_index('Class')['F1-Score']

        # Create DataFrame for improvements
        improvements = pd.DataFrame()

        for exp_name, metrics in all_class_metrics.items():
            if exp_name == 'No Augmentation':
                continue

            exp_f1 = metrics.set_index('Class')['F1-Score']
            f1_diff = exp_f1 - baseline_f1
            improvements[exp_name] = f1_diff

        # Create a summary showing the best augmentation strategy for each class
        best_strategy = pd.DataFrame(index=class_names, columns=['Best Strategy', 'F1 Score', 'Improvement'])

        for class_name in class_names:
            baseline_score = baseline_f1.loc[class_name]
            best_score = baseline_score
            best_exp = 'No Augmentation'

            for exp_name, metrics in all_class_metrics.items():
                if exp_name == 'No Augmentation':
                    continue

                score = metrics.set_index('Class').loc[class_name, 'F1-Score']
                if score > best_score:
                    best_score = score
                    best_exp = exp_name

            improvement = best_score - baseline_score
            best_strategy.loc[class_name] = [best_exp, best_score, improvement]

        # Calculate percent improvement
        best_strategy['% Improvement'] = (best_strategy['Improvement'] / baseline_f1) * 100

        # Sort by improvement
        best_strategy = best_strategy.sort_values('% Improvement', ascending=False)

        # Display the summary table
        print("\nBest Augmentation Strategy by Class:")
        print(best_strategy)

        # Save the summary table
        best_strategy.to_csv('best_augmentation_by_class.csv')

        # Create heatmap of confusion matrices for key experiments
        key_experiments = ['No Augmentation', 'Geometric Augmentation', '5% Synthetic Data', '5% Synthetic + Geometric']
        key_experiments = [exp for exp in key_experiments if exp in all_confusion_matrices]

        if len(key_experiments) > 0:
            fig, axes = plt.subplots(1, len(key_experiments), figsize=(5*len(key_experiments), 5))
            if len(key_experiments) == 1:
                axes = [axes]  # Make it iterable for single experiment case

            for i, exp_name in enumerate(key_experiments):
                conf_matrix = all_confusion_matrices[exp_name]
                axes[i].set_title(f'Confusion Matrix: {exp_name}')
                axes[i].set_xlabel('Predicted')
                axes[i].set_ylabel('True')
                plt.setp(axes[i].get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')
                plt.setp(axes[i].get_yticklabels(), rotation=45, ha='right', rotation_mode='anchor')

        return f1_comparison, improvements, best_strategy

    else:
        print("No baseline 'No Augmentation' experiment found for comparison.")
        return f1_comparison, None, None
    


import pickle
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_recall_fscore_support
def run_experiments():
    # Define transformations

    # 1. Basic transformation (no augmentation)
    transform_basic = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # 2. Geometric augmentation
    transform_geometric = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Standard test transformation (no augmentation)
    transform_test = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Create base datasets
    base_train_dataset = EuroSATDataset(train_val_dataset, train_indices, transform_basic)
    geo_train_dataset = EuroSATDataset(train_val_dataset, train_indices, transform_geometric)
    val_dataset = EuroSATDataset(train_val_dataset, val_indices, transform_test)

    # Create validation dataloader (common for all experiments)
    val_loader = DataLoader(val_dataset, batch_size=280, shuffle=False, num_workers=10)

    # Extract synthetic dataset
    print("Extracting synthetic dataset...")
#################################################################################################################################################################
    #### PICK THE CLAUDIA MODEL WE WANT (10, 50 OR 100)
    zip_path = "./data/ddpm-generated-images_10.zip"
    extract_path = "./data/ddpm_generated"
    if not os.path.exists(extract_path):
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)

    # Load synthetic data
    synthetic_full = ImageFolder(
        root=extract_path,
        transform=transform_basic
    )

    # Define different augmentation ratios
    synth_ratios = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50]

    # Store results
    all_results = {
        'experiment': [],
        'best_val_acc': []
    }

    # Store class-wise metrics for each experiment
    all_class_metrics = {}
    all_confusion_matrices = {}

    # Define class names
    class_names = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway',
                  'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']

    # Run experiment with no augmentation
    print("\n=========================================")
    print("Running experiment: No Augmentation")
    print("=========================================")

    train_loader = DataLoader(base_train_dataset, batch_size=280, shuffle=True, num_workers=10)

    model = ResNet18Classifier(num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)

    _, best_acc, class_metrics, conf_matrix = train_model(
        model, train_loader, val_loader, criterion, optimizer, scheduler,
        #save_path="/content/drive/MyDrive/models/resnet18_no_aug.pth"
    )

    all_results['experiment'].append("No Augmentation")
    all_results['best_val_acc'].append(best_acc)
    all_class_metrics["No Augmentation"] = class_metrics
    all_confusion_matrices["No Augmentation"] = conf_matrix

    # Visualize class metrics for this experiment
    visualize_class_metrics(class_metrics, conf_matrix, "No Augmentation", class_names)

    # Run experiment with geometric augmentation
    print("\n=========================================")
    print("Running experiment: Geometric Augmentation")
    print("=========================================")

    train_loader = DataLoader(geo_train_dataset, batch_size=280, shuffle=True, num_workers=10)

    model = ResNet18Classifier(num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)

    _, best_acc, class_metrics, conf_matrix = train_model(
        model, train_loader, val_loader, criterion, optimizer, scheduler,
        #save_path="/content/drive/MyDrive/models/resnet18_geo_aug.pth"
    )

    all_results['experiment'].append("Geometric Augmentation")
    all_results['best_val_acc'].append(best_acc)
    all_class_metrics["Geometric Augmentation"] = class_metrics
    all_confusion_matrices["Geometric Augmentation"] = conf_matrix

    # Visualize class metrics for this experiment
    visualize_class_metrics(class_metrics, conf_matrix, "Geometric Augmentation", class_names)

    # Run experiments with different synthetic data ratios
    for ratio in synth_ratios:
        exp_name = f"{int(ratio*100)}% Synthetic Data"
        print(f"\n=========================================")
        print(f"Running experiment: {exp_name}")
        print(f"=========================================")

        # Create dataset with synthetic data
        combined_dataset = create_synthetic_dataset(base_train_dataset, synthetic_full, ratio)
        train_loader = DataLoader(combined_dataset, batch_size=280, shuffle=True, num_workers=10)

        # Check class distribution
        #combined_labels = [label for _, label in combined_dataset]
        #df = plot_class_distribution(combined_labels, f"Class Distribution with {int(ratio*100)}% Synthetic Data")
        #print(df)

        model = ResNet18Classifier(num_classes=10).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)

        _, best_acc, class_metrics, conf_matrix = train_model(
            model, train_loader, val_loader, criterion, optimizer, scheduler,
            #save_path=f"/content/drive/MyDrive/models/resnet18_synth_{int(ratio*100)}pct.pth"
        )

        all_results['experiment'].append(exp_name)
        all_results['best_val_acc'].append(best_acc)
        all_class_metrics[exp_name] = class_metrics
        all_confusion_matrices[exp_name] = conf_matrix

        # Visualize class metrics for this experiment
        visualize_class_metrics(class_metrics, conf_matrix, exp_name, class_names)

    # Add experiments with combined geometric augmentation and synthetic data
    # Start with the most promising synthetic ratio (5% based on previous findings)
    combined_ratios = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50]

    for ratio in combined_ratios:
        exp_name = f"{int(ratio*100)}% Synthetic + Geometric"
        print(f"\n=========================================")
        print(f"Running experiment: {exp_name}")
        print(f"=========================================")

        # Create combined dataset with synthetic data
        combined_dataset = create_synthetic_dataset(geo_train_dataset, synthetic_full, ratio)
        train_loader = DataLoader(combined_dataset, batch_size=280, shuffle=True, num_workers=10)

        # Check class distribution
        #combined_labels = [label for _, label in combined_dataset]
        #df = plot_class_distribution(combined_labels, f"Class Distribution with {exp_name}")
        #print(df)

        model = ResNet18Classifier(num_classes=10).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)

        _, best_acc, class_metrics, conf_matrix = train_model(
            model, train_loader, val_loader, criterion, optimizer, scheduler,
            #save_path=f"/content/drive/MyDrive/models/resnet18_combined_{int(ratio*100)}pct.pth"
        )

        all_results['experiment'].append(exp_name)
        all_results['best_val_acc'].append(best_acc)
        all_class_metrics[exp_name] = class_metrics
        all_confusion_matrices[exp_name] = conf_matrix

        # Visualize class metrics for this experiment
        visualize_class_metrics(class_metrics, conf_matrix, exp_name, class_names)

    # Plot final overall comparison
    results_df = pd.DataFrame(all_results)
    # plt.figure(figsize=(12, 6))
    # sns.barplot(data=results_df, x='experiment', y='best_val_acc')
    # plt.title('Best Validation Accuracy by Experiment')
    # plt.ylabel('Validation Accuracy')
    # plt.xticks(rotation=45)
    # plt.tight_layout()
    # plt.savefig('overall_accuracy_comparison.png')
    # plt.show()

    print("Final results:")
    print(results_df)

    # Compare class-wise performance across experiments
    compare_class_performance(all_class_metrics, all_confusion_matrices, class_names)

    # Generate comprehensive analysis for thesis
    generate_thesis_analysis(all_results, all_class_metrics, all_confusion_matrices, class_names)

    return results_df, all_class_metrics, all_confusion_matrices

# Add this function to generate comprehensive analysis for your thesis
def generate_thesis_analysis(all_results, all_class_metrics, all_confusion_matrices, class_names):
    """
    Generate comprehensive analysis for thesis including tables and figures
    """
    # Create a directory for thesis outputs
    output_dir = "thesis_outputs"
    os.makedirs(output_dir, exist_ok=True)

    # 1. Create and save table of overall results
    results_df = pd.DataFrame(all_results)
    results_df.to_csv(f"{output_dir}/overall_accuracy.csv", index=False)

    # 2. Create table showing class-wise F1 scores for all experiments
    f1_comparison = pd.DataFrame(index=class_names)
    precision_comparison = pd.DataFrame(index=class_names)
    recall_comparison = pd.DataFrame(index=class_names)

    for exp_name, metrics_df in all_class_metrics.items():
        metrics_df = metrics_df.set_index('Class')
        f1_comparison[exp_name] = metrics_df['F1-Score']
        precision_comparison[exp_name] = metrics_df['Precision']
        recall_comparison[exp_name] = metrics_df['Recall']

    # Save tables for thesis
    f1_comparison.to_csv(f"{output_dir}/f1_scores_by_class.csv")
    precision_comparison.to_csv(f"{output_dir}/precision_by_class.csv")
    recall_comparison.to_csv(f"{output_dir}/recall_by_class.csv")

    # 3. For each class, identify which augmentation strategy works best
    if 'No Augmentation' in all_class_metrics:
        baseline_metrics = all_class_metrics['No Augmentation'].set_index('Class')

        # Create a table of best strategy for each class
        best_strategy = pd.DataFrame(index=class_names)
        best_strategy['Baseline F1'] = baseline_metrics['F1-Score']

        for class_name in class_names:
            max_f1 = baseline_metrics.loc[class_name, 'F1-Score']
            best_exp = 'No Augmentation'

            for exp_name, metrics_df in all_class_metrics.items():
                if exp_name == 'No Augmentation':
                    continue

                metrics_df = metrics_df.set_index('Class')
                f1 = metrics_df.loc[class_name, 'F1-Score']

                if f1 > max_f1:
                    max_f1 = f1
                    best_exp = exp_name

            best_strategy.loc[class_name, 'Best Strategy'] = best_exp
            best_strategy.loc[class_name, 'Best F1'] = max_f1
            best_strategy.loc[class_name, 'Improvement'] = max_f1 - baseline_metrics.loc[class_name, 'F1-Score']
            best_strategy.loc[class_name, '% Improvement'] = (best_strategy.loc[class_name, 'Improvement'] /
                                                            baseline_metrics.loc[class_name, 'F1-Score']) * 100

        # Sort by improvement
        best_strategy = best_strategy.sort_values('% Improvement', ascending=False)
        best_strategy.to_csv(f"{output_dir}/best_strategy_by_class.csv")

        # 4. Generate figures for most improved classes
        top_improved_classes = best_strategy.nlargest(3, '% Improvement').index

        for class_name in top_improved_classes:
            plt.figure(figsize=(10, 6))
            class_f1 = f1_comparison.loc[class_name]
            class_f1.plot(kind='bar')
            plt.title(f'F1 Score for {class_name} Across Augmentation Strategies')
            plt.ylabel('F1 Score')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f"{output_dir}/{class_name}_f1_comparison.png")
            plt.close()

        # 5. Generate figures for least improved classes
        bottom_improved_classes = best_strategy.nsmallest(3, '% Improvement').index

        for class_name in bottom_improved_classes:
            plt.figure(figsize=(10, 6))
            class_f1 = f1_comparison.loc[class_name]
            class_f1.plot(kind='bar')
            plt.title(f'F1 Score for {class_name} Across Augmentation Strategies')
            plt.ylabel('F1 Score')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f"{output_dir}/{class_name}_f1_comparison.png")
            plt.close()

    # 6. Create a heatmap showing F1 score improvements for all classes across all experiments
    plt.figure(figsize=(14, 10))

    # Calculate improvements over baseline
    improvements_df = f1_comparison.copy()

    if 'No Augmentation' in improvements_df.columns:
        baseline = improvements_df['No Augmentation']
        for col in improvements_df.columns:
            if col != 'No Augmentation':
                improvements_df[col] = improvements_df[col] - baseline

        # Drop the baseline column
        improvements_df = improvements_df.drop(columns=['No Augmentation'])

    # 7. Generate summary statistics
    summary = pd.DataFrame(index=['Mean Improvement', 'Max Improvement', 'Class with Max Improvement'])

    for col in improvements_df.columns:
        summary.loc['Mean Improvement', col] = improvements_df[col].mean()
        summary.loc['Max Improvement', col] = improvements_df[col].max()
        summary.loc['Class with Max Improvement', col] = improvements_df[col].idxmax()

    summary.to_csv(f"{output_dir}/summary_statistics.csv")

    print(f"Thesis analysis materials saved to {output_dir}/")


num_runs = 10
all_accuracy_results = []
all_f1_results = []
all_conf_matrices = []

for run_id in range(1, num_runs + 1):
    print(f"\n Starting Run {run_id}")
    results_df, class_metrics_dict, confusion_matrices_dict = run_experiments()

    # 1. Save overall accuracy
    results_df["run_id"] = run_id
    all_accuracy_results.append(results_df)

    # 2. Save per-class F1/precision/recall
    for exp_name, class_df in class_metrics_dict.items():
        class_df_copy = class_df.copy()
        class_df_copy["experiment"] = exp_name
        class_df_copy["run_id"] = run_id
        all_f1_results.append(class_df_copy)

    # 3. Save confusion matrices
    for exp_name, matrix in confusion_matrices_dict.items():
        df_matrix = pd.DataFrame(matrix)
        df_matrix["experiment"] = exp_name
        df_matrix["run_id"] = run_id
        all_conf_matrices.append(df_matrix)

# --- Combine & Save ---
if not os.path.exists("results"):
    os.makedirs("results")

# 1. Accuracies
acc_df = pd.concat(all_accuracy_results, ignore_index=True)
acc_df.to_csv("results/accuracy_across_runs.csv", index=False)

# 2. F1 scores
f1_df = pd.concat(all_f1_results, ignore_index=True)
f1_df.to_csv("results/f1_across_runs.csv", index=False)

# 3. Confusion matrices
conf_df = pd.concat(all_conf_matrices, ignore_index=True)
conf_df.to_csv("results/confusion_across_runs.csv", index=False)

print(" All data saved.")
